<!DOCTYPE html>
<html lang="en">

<head>
  <title> Scrapecrow - Web Scraping Target Discovery: Crawling</title>
  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="description" content="Educational blog about web-scraping, crawling and related data extraction subjects" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css"
    integrity="sha256-XoaMnoYC5TH6/+ihMEnospgm0J1PM/nioxbOUdnM8HY=" crossorigin="anonymous">
  <script src="https://scrapecrow.com/theme/main.js"></script>
  <link rel="stylesheet" href="https://scrapecrow.com/theme/css/main.css" />
  <link rel="stylesheet" href="https://scrapecrow.com/theme/css/applause-button.css" />
  <script src="https://scrapecrow.com/theme/applause-button.js"></script>
  <link href="Scrapecrow/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
    title="Scrapecrow Full Atom Feed" />
  <link href="Scrapecrow/atom.xml" type="application/atom+xml" rel="alternate"
    title="Scrapecrow Atom Feed" />
  <link href="Scrapecrow/rss.xml" type="application/rss+xml" rel="alternate"
    title="Scrapecrow RSS Feed" />
  <link href="Scrapecrow/feeds/{slug}.atom.xml" type="application/atom+xml"
    rel="alternate" title="Scrapecrow Categories Atom Feed" />

<meta name="author" content="Bernardas Ališauskas" />
<meta name="description" content="The most common web scraping target discovery technique: recursive crawling. How does it work? What are the pros and cons and the most optimal execution patterns?" />
  <meta property="og:site_name" content="Scrapecrow"/>
  <meta property="og:title" content="Web Scraping Target Discovery: Crawling"/>
  <meta property="og:description" content="The most common web scraping target discovery technique: recursive crawling. How does it work? What are the pros and cons and the most optimal execution patterns?"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://scrapecrow.com/web-scraping-discovery-crawling.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2021-09-28 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2021-09-28 00:00:00+02:00"/>
  <meta property="article:author" content="Bernardas Ališauskas">
  <meta property="article:section" content="articles"/>
  <meta property="article:tag" content="discovery"/>
  <meta property="article:tag" content="crawling"/>
  <meta property="article:tag" content="intermediate"/>
  <meta property="article:tag" content="discovery-methods"/>
  <meta property="og:image" content="https://scrapecrow.com/images/logo-og.png">
</head>

<body>
  <div class="navigation">
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a class="navbar-item" href="https://scrapecrow.com">
          <img src="/images/logo.svg" width="50"></img>
        </a>
        <a class="navbar-item" href="https://scrapecrow.com">Scrapecrow</a>
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu" id="navMenu">
        <div class="navbar-start">
          <a class="navbar-item" href="/pages/about.html">about</a>
          <a class="navbar-item" href="/pages/hire.html">hire</a>
          <a class="navbar-item" href="/pages/coffee.html">☕</a>
          <a class="navbar-item" href="https://matrix.to/#/#web-scraping:matrix.org">#web-scraping on matrix</a>
        </div>
      </div>
    </nav>
  </div>
  <div class="main">
<div class="content">
  <div class="post-meta">
<div class="post-meta">
    <a href="https://scrapecrow.com/category/articles.html" class="category">articles</a>
    &mdash;
    <span title="2021-09-28T00:00:00+02:00">Tue 28 September 2021</span></br>
    <a class="tag" href="https://scrapecrow.com/tag/discovery.html" title="finding content on a website">discovery</a>
    <a class="tag" href="https://scrapecrow.com/tag/crawling.html" title="programatically following web links to discover content">crawling</a>
    <a class="tag" href="https://scrapecrow.com/tag/intermediate.html" title="intermediate level article">intermediate</a>
    <a class="tag" href="https://scrapecrow.com/tag/discovery-methods.html" title="ways to discover content on a website">discovery-methods</a>
    </br>
</div>  </div>
  <h1>Web Scraping Target Discovery: Crawling</h1>
  <div class="post-content">
    <!--insert table of contents between text and first header-->
    <p><a href="/images/banner-web.jpg"><img class="fullc" loading="lazy" src="/images/banner-web.jpg" title=""/></a><figcaption></figcaption></p>
<p>Most web scrapers are made up of two core parts: finding products on the website and actually scraping them. The former is often referred to as "target discovery" step. For example to scrape product data of an e-commerce website we would need to find urls to each individual product and only then we can scrape their data.</p>
<p>Discovering targets to scrape in web scraping is often a challenging and important task. This series of blog posts tagged with <a href="/tag/discovery-methods.html">#discovery-methods</a> (also see <a href="/web-scraping-discovery.html">main article</a>) covers common target discovery approaches.</p>
<p>In this article we'll take a look at web crawling and how can we use it as a discovery strategy in web scraping.  </p>

    <hr>
    <div class="pure-u">
      <div id="toc"><ul><li><a class="toc-href" href="#what-is-recursive-crawling-and-how-is-it-used-in-web-scraping" title="What is recursive crawling and how is it used in web-scraping?">What is recursive crawling and how is it used in web-scraping?</a></li><li><a class="toc-href" href="#example-use-case-hmcom" title="Example Use Case: hm.com">Example Use Case: hm.com</a><ul><li><a class="toc-href" href="#crawling-rules" title="Crawling Rules">Crawling Rules</a></li><li><a class="toc-href" href="#crawl-loop" title="Crawl Loop">Crawl Loop</a></li><li><a class="toc-href" href="#link-extracting" title="Link Extracting">Link Extracting</a></li><li><a class="toc-href" href="#putting-it-all-together" title="Putting It All Together">Putting It All Together</a></li></ul></li><li><a class="toc-href" href="#summary-and-further-reading_1" title="Summary and Further Reading">Summary and Further Reading</a></li></ul></div>
    </div>
    <hr>
    <h2 id="what-is-recursive-crawling-and-how-is-it-used-in-web-scraping">What is recursive crawling and how is it used in web-scraping?</h2>
<p>One of the most common ways to discover web scraping targets is to recursively crawl the website. This technique is usually used by broad scrapers (scrapers that scrape many different websites) and index crawlers such as Google and other search engine bots.<br/>
In short crawling is recursive scraping technique where given a start url and some crawling rules the scraper continues exploring the website by visiting <em>all'ish</em> of the links present on the website.  </p>
<p>To wrap our heads around crawling concept easier lets refer to this small flow chart:</p>
<p><a href="/images/crawl-flow.png"><img class="" loading="lazy" src="/images/crawl-flow.png" width='title=""'/></a><figcaption></figcaption></p>
<p>This flow chart illustrates the simplest domain-bound crawl spider flow: the crawler is given a starting point, it scrapes and parses it for urls present in the html body. Then applies matching rules to urls and determines whether to save to urls (for scraping later) or whether to follow them up to repeat the whole process.</p>
<p>Before using crawling as a web scraping discovery strategy it's a good practice to reflect on common pros and cons of this technique and see whether that would fit your web-scraping project:</p>
<p>Pros:  </p>
<ul>
<li><strong>Generic Algorithm</strong>: can be applied to any website with few adjustments. In other words one web scraper can be adapted to any website quite easily.  </li>
<li><strong>Good Coverage</strong>: some websites (like e-commerce) are well interlinked thus crawling will have great discovery coverage.  </li>
<li><strong>Easy to Develop</strong>: no reverse-engineering skills are required since we're just falling natural website structure.  </li>
</ul>
<p>Cons:  </p>
<ul>
<li><strong>Inefficient and Slow</strong>: since crawling is a very generic solution it comes with a lot of inefficiencies. Often extracted links might not contain any product links so lots of crawl branches end up in dead ends.  </li>
<li><strong>Insufficient Coverage</strong>: some websites are not well interlinked (sometimes purposefully to prevent web scrapers). Crawlers can't discover items that are not referenced anywhere.  </li>
<li><strong>Risk</strong>: since scraped link bandwidth is much bigger than other discovery approaches the scrapers IPs are more likely to be throttled or blocked.  </li>
<li><strong>Struggles With Javascript Heavy Websites</strong>: since crawling is very generic and web scrapers don't execute javascript content (unless using browser emulation) some websites might be too complex for web scraper to follow.  </li>
</ul>
<p>We can see that crawling is a smart generic way to discover scrape targets however it's not without it's faults: it's slower, less accurate and might be hard to accomplish with some javascript heavy websites.<br/>
Lets take a look at example target discovery implementation that uses web crawling.</p>
<h2 id="example-use-case-hmcom">Example Use Case: hm.com</h2>
<p>Lets take a look at a popular clothing e-commerce website: <a href="https://hm.com">https://hm.com</a>. We'll be using crawling approach to find all clothing products on the website. </p>
<p>First lets establish essential parts that make up a web crawler:</p>
<ol>
<li>Link extractor - a function/object that can find urls in html body.</li>
<li>Defined link pattern rules to follow - a function/object that determines how to handle up extracted links.</li>
<li>Duplicate filter - object that keeps track of links scraper visited.</li>
<li>Limiter - since crawling visits many urls we need to limit connection rate to not overwhelm the website.  </li>
</ol>
<p>These are 4 components that make up a basic web crawler. Lets see how we can implement them for hm.com.</p>
<h3 id="crawling-rules">Crawling Rules</h3>
<p>First lets establish our crawling rules. As per above flowchart our crawler needs to know which urls to follow up and which to save:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">re</span>

<span class="k">class</span> <span class="nc">HMScraper</span><span class="p">:</span>
    <span class="n">save_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"/productpage\."</span><span class="p">)</span>  <span class="c1"># e.g ...com/en_us/productpage.09008.html</span>
    <span class="n">follow_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"\.html"</span><span class="p">)</span>
    <span class="n">follow_saved_urls</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>
</td></tr></table>
<p>Here we defined our crawling rules:</p>
<ul>
<li>We want to save all urls that contain <code>/productpage.</code> in the url as all hm.com products follow this pattern  </li>
<li>We want to follow up any url containing <code>.html</code> </li>
<li>
<p>Do not follow urls that are being saved.  </p>
<p class="info">Following saved urls can useful as product pages often contain "related products" urls which can help us increase discovery coverage. For hm.com domain this is unnecessary.<br/></p>
</li>
</ul>
<p>These are 3 rules that define our crawler's routine for domain <code>hm.com</code>. With that ready lets take a look how we can create a link extractor function that will use these rules to extract crawl targets.</p>
<h3 id="crawl-loop">Crawl Loop</h3>
<p>Having crawling rules defined we need to create a crawl loop that uses these rules to schedule a whole crawl process. <br/>
In this example for our http processing we'll be using <code>httpx</code> and for html parsing <code>parsel</code> python packages. With these two tools we can define basic crawler skeleton:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urlparse</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">parsel</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="kn">from</span> <span class="nn">httpx</span> <span class="kn">import</span> <span class="n">AsyncClient</span>


<span class="k">class</span> <span class="nc">HMScraper</span><span class="p">:</span>
    <span class="n">save_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"/productpage\."</span><span class="p">)</span>  <span class="c1"># e.g ...com/en_us/productpage.09008.html</span>
    <span class="n">follow_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"\.html"</span><span class="p">)</span>
    <span class="n">follow_saved_urls</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="c1"># asyncio.Semaphore object allows us to limit coroutine concurrency </span>
        <span class="c1"># in our case we can limit how many concurrent requests are being made</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limiter</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="n">limit</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="fm">__aenter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""on scraper creation open http session"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">AsyncClient</span><span class="p">(</span>
            <span class="c1"># we should use a browser-like user agent header to avoid being blocked</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">"User-Agent"</span><span class="p">:</span> <span class="s2">"Mozilla/5.0 (X11; Linux x86_64; rv:92.0) Gecko/20100101 Firefox/92.0"</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">async</span> <span class="k">def</span> <span class="fm">__aexit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sd">"""on scraper destruction close http session"""</span>
        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">aclose</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">"""our http request wrapper function that implements rate limiting"""</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">limiter</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">e</span>
            <span class="k">return</span> <span class="n">resp</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
        <span class="c1"># for display purposes lets just print the url</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">scrape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">find_links</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="o">...</span>
</code></pre></div>
</td></tr></table>
<p>With this skeleton, we have basic usage API for our scraper. We can define our run function:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">HMScraper</span><span class="p">()</span> <span class="k">as</span> <span class="n">scraper</span><span class="p">:</span>
        <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># homepage for US website</span>
            <span class="s2">"https://www2.hm.com/en_us/index.html"</span>
        <span class="p">]</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">run</span><span class="p">())</span>
</code></pre></div>
</td></tr></table>
<p>Great! Now all we have to do is fill in the interesting bits: link extraction and scrape loop.<br/>
For scrape loop all we need to do is request urls, find links in them, follow or save ones that match our rules:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>    <span class="k">async</span> <span class="k">def</span> <span class="nf">scrape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="sd">"""Breadth first"""</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">to_follow</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">resp</span> <span class="ow">in</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">as_completed</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_request</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span>
                <span class="c1"># skip failed requests; ideally this should be retried or logged</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">resp</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_links</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">text</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_urls</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
                        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">follow_saved_urls</span><span class="p">:</span>
                            <span class="k">continue</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">follow_urls</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  following </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                        <span class="n">to_follow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">to_follow</span><span class="p">:</span>
                <span class="n">urls</span> <span class="o">=</span> <span class="n">to_follow</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span>  <span class="c1"># end of the crawl</span>
</code></pre></div>
</td></tr></table>
<p>Here we've defined an "endless" while loop that does exactly that: get htmls, parse them for urls where we store some of them and follow up the others. The last remaining piece is our link extraction logic.   </p>
<h3 id="link-extracting">Link Extracting</h3>
<p>Link extraction process is the core part that makes the crawler and can get quite complex in logic. For our example domain <code>hm.com</code> it's relatively simple. We'll find all urls in the page by following <code>&lt;a&gt;</code> nodes:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>    <span class="k">def</span> <span class="nf">find_links</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resp</span><span class="p">,</span> <span class="n">only_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># build a parsable tree from html body</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="n">current_url_parts</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
        <span class="c1"># find all &lt;a&gt; nodes and select their href attribute</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">"//a/@href"</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="c1"># convert relative url to absolute</span>
            <span class="k">if</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"/"</span><span class="p">):</span>
                <span class="n">url</span> <span class="o">=</span> <span class="n">current_url_parts</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">geturl</span><span class="p">()</span>
            <span class="c1"># skip absolute urls that do not match current domain</span>
            <span class="k">if</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">netloc</span> <span class="o">!=</span> <span class="n">current_url_parts</span><span class="o">.</span><span class="n">netloc</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># skip visited urls</span>
            <span class="k">if</span> <span class="n">only_unique</span> <span class="ow">and</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">url</span>
</code></pre></div>
</td></tr></table>
<p>Here we first build a tree parser object to get all those <code>&lt;a&gt;</code> node links. Then we iterate through them and filter out anything that is not an url of this website or has been visited already.</p>
<p class="info">Link Extraction can get complicated very quickly as some website can contain non-html files (e.g. <code>/document.pdf</code>) that need to be filtered out and many other niche scenarios. </p>
<p>With link extraction complete, we can put together our whole crawler into once piece and see how it performs!</p>
<h3 id="putting-it-all-together">Putting It All Together</h3>
<p>Now that we have all parts complete: crawl loop, link extraction, link matching and request limiting. Let's put it all together and run it:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urlparse</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">parsel</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="kn">from</span> <span class="nn">httpx</span> <span class="kn">import</span> <span class="n">AsyncClient</span>


<span class="k">class</span> <span class="nc">HMScraper</span><span class="p">:</span>
    <span class="n">save_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"/productpage\."</span><span class="p">)</span>  <span class="c1"># e.g ...com/en_us/productpage.09008.html</span>
    <span class="n">follow_urls</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"\.html"</span><span class="p">)</span>
    <span class="n">follow_saved_urls</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limiter</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="n">limit</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="fm">__aenter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""on scraper creation open http session"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">AsyncClient</span><span class="p">(</span>
            <span class="c1"># we should use a browser-like user agent header to avoid being blocked</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">"User-Agent"</span><span class="p">:</span> <span class="s2">"Mozilla/5.0 (X11; Linux x86_64; rv:92.0) Gecko/20100101 Firefox/92.0"</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">async</span> <span class="k">def</span> <span class="fm">__aexit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sd">"""on scraper destruction close http session"""</span>
        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">aclose</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">limiter</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">e</span>
            <span class="k">return</span> <span class="n">resp</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">scrape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="sd">"""Breadth first"""</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">to_follow</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">resp</span> <span class="ow">in</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">as_completed</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_request</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]):</span>
                <span class="n">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="ne">Exception</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">resp</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_links</span><span class="p">(</span><span class="n">resp</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_urls</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
                        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">follow_saved_urls</span><span class="p">:</span>
                            <span class="k">continue</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">follow_urls</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  following </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                        <span class="n">to_follow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">to_follow</span><span class="p">:</span>
                <span class="n">urls</span> <span class="o">=</span> <span class="n">to_follow</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span>  <span class="c1"># end of the crawl</span>

    <span class="k">def</span> <span class="nf">find_links</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resp</span><span class="p">,</span> <span class="n">only_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        find all relative page links in html link nodes</span>
<span class="sd">        """</span>
        <span class="c1"># build a parsable tree from html body</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="n">current_url_parts</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
        <span class="c1"># find all &lt;a&gt; nodes and select their href attribute</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">"//a/@href"</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="c1"># convert relative url to absolute</span>
            <span class="k">if</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"/"</span><span class="p">):</span>
                <span class="n">url</span> <span class="o">=</span> <span class="n">current_url_parts</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">geturl</span><span class="p">()</span>
            <span class="c1"># skip absolute urls that do not match current domain</span>
            <span class="k">if</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">netloc</span> <span class="o">!=</span> <span class="n">current_url_parts</span><span class="o">.</span><span class="n">netloc</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># skip visited urls</span>
            <span class="k">if</span> <span class="n">only_unique</span> <span class="ow">and</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seen_urls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">url</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">HMScraper</span><span class="p">()</span> <span class="k">as</span> <span class="n">scraper</span><span class="p">:</span>
        <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># homepage for US website</span>
            <span class="s2">"https://www2.hm.com/en_us/index.html"</span>
        <span class="p">]</span>
        <span class="k">await</span> <span class="n">scraper</span><span class="o">.</span><span class="n">scrape</span><span class="p">(</span><span class="n">start_urls</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">run</span><span class="p">())</span>
</code></pre></div>
</td></tr></table>
<p>If we run our crawler we'll notice few things:
- At time of writing 13800~ results are being found which matches well with our other <a href="/tag/discovery-methods.html">#discovery-methods</a> used in this blog series.
- It took a while to complete this crawl: TODO second. Since we are crawling so many pages compared to other discovery methods we crawl </p>
<p>Finally, we can see that we can easily reuse most of this scraper for other websites, all we need to do is change our rules! That's the big selling point of crawlers, is that they're less domain specific than individual web scrapers.</p>
<h2 id="summary-and-further-reading_1">Summary and Further Reading</h2>
<p>To summarize, web crawling is a great discovery technique that lends easily to generic/broad scraper development because the same scrape loop can be applied to many targets just with some rule adjustments. However it's less efficient - slower and riskier when it comes to blocks - than other discovery techniques like <a href="/web-scraping-discover-search.html">Search Bar</a> or <a href="/web-scraping-discover-sitemaps.html">Sitemaps</a>. </p>
<hr/>
<p>For more web-scraping discovery techniques, see <a href="/tag/discovery-methods.html">#discovery-methods</a> and <a href="/tag/discovery.html">#discovery</a> for more discovery related subjects.  </p>
<p>If you have any questions, come join us on <a href="https://matrix.to/#/%23web-scraping:matrix.org">#web-scraping on matrix</a>, check out <a href="https://stackoverflow.com/questions/tagged/web-scraping">#web-scraping on stackoverflow</a> or leave a comment below!  </p>
<p>As always, you can hire me for web-scraping consultation over at <a href="/pages/hire.html">hire</a> page and happy scraping!  </p>
<p class="info">The code used in this article can be found on <a href="https://github.com/Granitosaurus/scrapecrow/tree/main/examples">github</a>.<br/></p>
  </div>
</div>
<hr>
<div class="article-buttons">
  <div class="float-side applause" title="show your appreciation">
    <applause-button url="https://scrapecrow.com/web-scraping-discovery-crawling.html" style="width: 58px; height: 58px;" />
  </div>
  <div class="article-button applause" title="show your appreciation">
    <applause-button url="https://scrapecrow.com/web-scraping-discovery-crawling.html" style="width: 58px; height: 58px;" />
  </div>
  <div class="article-button twitter" title="share on twitter">
    <a href="http://twitter.com/share?text=&url=https://scrapecrow.com/web-scraping-discovery-crawling.html&hashtags=">
      <i class="fa fa-twitter fa-3x pure-menu-link" aria-hidden="true"></i>
    </a>
  </div>
  <div class="article-button email">
    <a href="https://716df175.sibforms.com/serve/MUIEALpKPp8WjHrVwQOX6keZXLkJRbnFEh2y6YhTmVmT4Z0Khgbi2MFvPO1OObOrjbMi_S0M7VXkXGkcbh36H-SqEwM3dHXxdrOOXwEPGcp9rTQKkQvMkC70Dq9RmCoikia87nLsRcx0VVGmCG2zyx5s8BwpqevRmh70vKSaLe7e95yZDCROMvm2HcN3UpLw7UsFxl_UbI6TjY_e" title="subscribe to mailist">
      <i class="fa fa-email-bulk-o fa-3x pure-menu-link" aria-hidden="true"></i>
    </a>
  </div>
</div>
<hr>
<div class="comments">
  <script src="https://utteranc.es/client.js" repo="granitosaurus/scrapecrow"
    issue-term="web-scraping-discovery-crawling" label="comments"
    theme="github-light" crossorigin="anonymous" async>
  </script>
</div>
  </div>
  <footer class="footer">
    <div class="content">
      <span class="footer-item"><a href="/archives.html">Archives</a></span>
      <span class="footer-item"><a href="/tags.html">Tags</a></span>
      <span class="footer-item"><a href="https://blog.getpelican.com/">Made with Pelican</a></span>
      <span class="footer-item"><a href="https://github.com/granitosaurus/scrapecrow/">Source on Github</a></span>
      <span class="footer-item"><a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
      <span class="footer-item">
        <a href="atom.xml">
          <i class="fa fa-rss-square" aria-hidden="true"></i>
        </a>
      </span>
    </div>
  </footer>
<script data-goatcounter="https://scrapecrow.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>

</html>