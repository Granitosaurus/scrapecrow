Title: Web Scraping Target Discovery: Sitemaps
Date: 2021-09-19
Tags: discovery, discovery-methods, python, scraping, sitemap
Slug: web-scraping-discovery-sitemaps
Summary: There are many techniques when it comes to discovery web-scraping targets. One of the most common ones is to use website sitemap indexes. What are they and to take advantage of them in web-scraping?
toc: True
add_toc: True

Target discovery in web-scraping is how the scraper explores target website to find scraping targets. For example to scrape product data of an e-commerce website we would need to find urls to each individual product and scrape it's data. This step is called "discovery".

Discovering targets to scrape in web scraping is often a challenging and important  task. This series of blog posts covers common target discovery approaches, click this tag for more [#discovery-methods]

In this article will cover one particular discovery method of using website sitemaps to find our scrape targets.

## What are sitemaps and how are they used in web-scraping?

Sitemap is an index document generated by websites for web crawlers and indexers. For example websites that want to be crawled by google provide an index of their products so Google's crawlers can index it quicker. 

Sitemaps files are always of xml type (often gzip compressed) documents that contain URL locations and some meta information about them:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
   <url>
      <loc>http://www.example.com/</loc>
      <lastmod>2005-01-01</lastmod>
      <changefreq>monthly</changefreq>
      <priority>0.8</priority>
   </url>
</urlset> 
```

<info>
for more on sitemap structure rules see [official specification page](https://www.sitemaps.org/protocol.html)
</info>

The documents themselves are usually categorized by names so for example:   

- blog post of the website would be contained in `sitemap_blogs.xml`.   
- Sold products might be separated in multiple files of `sitemap_products_1.xml`, `sitemap_products_2.xml` etc  

Before using sitemaps a web scraping discovery strategy it's a good practice to reflect on common pros and cons of this technique and see whether that would fit a web-scraping project:

Pros:  

* __Efficiency__: Single sitemap can contain thousands of items and often entire catalog can be discovered in few requests!   
* __Simplicity__: There's no need for advanced reverse engineering knowledge to use sitemap based discovery.  

Cons:  

- __Data Staleness__: As they are often generated once a day, week or even month.  
- __Data Validity__: As per previous point because of sitemap staleness some product links might be expired. This might cause unnecessary load on your scraper.  
- __Data Completeness__: Since sitemaps are generated for crawlers and indexers they might not have all data that is available on the website. For this reason it is important to confirm sitemap coverage during the development of a scraper.  
- __Availability__: Sitemaps used to be an important part of the web, particularly used in SEO however they are not always present in modern websites that either try to avoid web-scraping or use hard-to-index website structures.  

Generally when developing discovery strategy sitemaps is the first place I look for product data, then confirm quality by trying alternative discovery approaches and seeing if coverage matches. 

## Finding Sitemaps

To take advantage of sitemaps we first need to figure how to find them. Common way to find sitemaps is checking `robots.txt` or `sitemaps.xml` file.  
For example lets take popular clothing shop `shein.com`:

First we would go to `/robots.txt` page: <https://shein.com/robots.txt>
```
User-agent: *

Disallow: /abt/userinfo
...

Sitemap: https://us.shein.com/sitemap-index.xml
```

<info>you might see different localization sub-domain instead of `us.shein` depending where you're connecting from - it's all the same in the context of this article</info>

We see some robot scraping rules and a link to the sitemap index! If we proceed to <https://us.shein.com/sitemap-index.xml> we can see:

```xml
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
	<sitemap>
	<loc>https://us.shein.com/sitemap-category-sc.xml</loc>
	<lastmod>2021-08-26</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-article.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-campaign.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-pages.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-category-sp.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-category-sets.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-category-c.xml</loc>
	<lastmod>2021-08-29</lastmod>
<sitemap>
	<loc>https://us.shein.com/sitemap-products-1.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
<sitemap>
	<loc>https://us.shein.com/sitemap-products-2.xml</loc>
	<lastmod>2021-08-29</lastmod>
</sitemap>
```

This is index of sitemap indexes. We see there are indexes for articles, pages and categories etc. - but most importantly product index: `.../sitemap-products-N.xml`.   
To add there's some important metadata as well: when indexes were last updated `<lastmod>`. In this case the index is 2 days old so this discovery approach will not pick up any products that have been added since.

## Example Use Case

Lets write a simple sitemap scraper that will find all product urls on previously mentioned website <https://shein.com>. For this we'll be using python with `requests` and `parsel` libraries:

```python
# requires:
# pip install requests parsel
import re
from typing import List

import requests
from parsel import Selector


def parse_sitemap(url: str) -> List[str]:
    """scrape sitemap and item urls from a sitemap link"""
    print(f"scraping: {url}")
    resp = requests.get(url)
    # turn html text to a parsable tree object
    doc_tree = Selector(resp.text)
    # find all <sitemap> nodes and take their urls
    other_sitemaps = doc_tree.xpath("//sitemap/text()").getall()
    # find all <loc> nodes and take their text (which is an url)
    urls = doc_tree.xpath("//loc/text()").getall()
    return other_sitemaps + urls


product_urls = set()
sitemap_directory = "https://us.shein.com/sitemap-index.xml"
for url in parse_sitemap(sitemap_directory):
    if "-products-" not in url:
        continue
    for url in parse_sitemap(url):
        # product urls match pattern com/<some product naming>.html
        # skip non-product urls
        if not re.search(r"shein.com/.+?\.html", url):
            continue
        product_urls.add(url)
print("\n".join(product_urls))
print(len(product_urls))
```
<info>full code with more comments and bonuses can be found on [github][code-github]</info>

If we run this small scraper script we'll see that this sitemap discovery approach will yield us over 802_000 unique results at the time of writing! Even if we scrape synchronously sitemap approach is a really efficient way to discover large amount of products.

Finally for a quality web-scraper it's a good practice to confirm whether this discovery approach has good coverage and will find us all products we need.

Finally we should confirm whether this discovery approach has good coverage by comparing it with other discovery approaches. For that we either need to find all product count number somewhere on the website or use another discovery strategy to evaluate our coverage. For this particular website we can take a look at [search bar discovery approach][searchbar]:

[% image src="shein.com-searchbar.png" %]

Using empty search approach described in [search discovery article][searchbar] we can see that our sitemap discovery coverage is missing 20_000 potentials results! This search number might not actually be true or our sitemap parsing code might be faulty. Nevertheless this is a good illustration of different discovery techniques and their importance. For important scrapers it's a good idea to diversify.

## Summary and Further Reading

To summarize using sitemaps in web scraping is an efficient, effective and quick product discovery technique with only real down-sides being data staleness, coverage and availability.

<hr>

For more web-scraping discovery techniques see [#discovery-methods] and [#discovery] for more discovery related subjects.  
The code used in this article can be found on [github][code-github]. 
If you're stuck with a web-scraping problem visit [#web-scraping on matrix] or [hire me] :)

[#discovery-methods]: /tag/discovery-methods.html
[#discovery]: /tag/discovery.html
[searchbar]: /web-scraping-discovery-search.html
[code-github]: https://github.com/Granitosaurus/scrapecrow/blob/main/examples/discovery-sitemaps.py
[#web-scraping on matrix]: https://matrix.to/#/%23web-scraping:matrix.org
[hire me]: /hire.html